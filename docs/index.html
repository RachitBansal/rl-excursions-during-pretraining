<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link
      rel="preload"
      href="./fonts/ibm_plex_sans/IBMPlexSans-Regular.ttf"
      as="font"
      type="font/ttf"
      crossorigin
    />
    <link
      rel="preload"
      href="./fonts/newsreader/Newsreader_14pt-Italic.ttf"
      as="font"
      type="font/ttf"
      crossorigin
    />
    <!-- Favicon: blog icon -->
    <link rel="icon" type="image/png" href="./assets/figures/icon.png" />
    <!-- Keep .ico as a fallback for older clients -->
    <!-- <link rel="icon" href="./favicon.ico" /> -->
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-ZSV2YDNY0W"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-ZSV2YDNY0W", { send_page_view: false });
    </script>
    <script>
      if (window.location.hostname === "localhost") {
        window.gtag = () => {};
      }
    </script>
    
		<link href="./_app/immutable/assets/0.C37RnVmk.css" rel="stylesheet">
		<link href="./_app/immutable/assets/rl_excursions.D57fG7ri.css" rel="stylesheet">
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_AMS-Regular.BQhdFMY1.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_AMS-Regular.DMm9YOAa.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_AMS-Regular.DRggAlZN.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Caligraphic-Bold.Dq_IR9rO.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Caligraphic-Bold.BEiXGLvX.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Caligraphic-Bold.ATXxdsX0.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Caligraphic-Regular.Di6jR-x-.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Caligraphic-Regular.CTRA-rTL.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Caligraphic-Regular.wX97UBjC.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Fraktur-Bold.CL6g_b3V.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Fraktur-Bold.BsDP51OF.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Fraktur-Bold.BdnERNNW.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Fraktur-Regular.CTYiF6lA.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Fraktur-Regular.Dxdc4cR9.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Fraktur-Regular.CB_wures.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Main-Bold.Cx986IdX.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Main-Bold.Jm3AIy58.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Main-Bold.waoOVXN0.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Main-BoldItalic.DxDJ3AOS.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Main-BoldItalic.SpSLRI95.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Main-BoldItalic.DzxPMmG6.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Main-Italic.NWA7e6Wa.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Main-Italic.BMLOBm91.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Main-Italic.3WenGoN9.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Main-Regular.B22Nviop.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Main-Regular.Dr94JaBh.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Main-Regular.ypZvNtVU.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Math-BoldItalic.CZnvNsCZ.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Math-BoldItalic.iY-2wyZ7.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Math-BoldItalic.B3XSjfu4.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Math-Italic.t53AETM-.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Math-Italic.DA0__PXp.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Math-Italic.flOr_0UB.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_SansSerif-Bold.D1sUS0GD.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_SansSerif-Bold.DbIhKOiC.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_SansSerif-Bold.CFMepnvq.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_SansSerif-Italic.C3H0VqGB.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_SansSerif-Italic.DN2j7dab.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_SansSerif-Italic.YYjJ1zSn.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_SansSerif-Regular.DDBCnlJ7.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_SansSerif-Regular.CS6fqUqJ.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_SansSerif-Regular.BNo7hRIc.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Script-Regular.D3wIWfF6.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Script-Regular.D5yQViql.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Script-Regular.C5JkGWo-.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Size1-Regular.mCD8mA8B.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Size1-Regular.C195tn64.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Size1-Regular.Dbsnue_I.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Size2-Regular.Dy4dx90m.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Size2-Regular.oD1tc_U0.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Size2-Regular.B7gKUWhC.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Size3-Regular.CTq5MqoE.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Size3-Regular.DgpXs0kz.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Size4-Regular.Dl5lxZxV.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Size4-Regular.BF-4gkZK.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Size4-Regular.DWFBv043.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="./_app/immutable/assets/KaTeX_Typewriter-Regular.CO6r4hn1.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="./_app/immutable/assets/KaTeX_Typewriter-Regular.C0xS9mPB.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="./_app/immutable/assets/KaTeX_Typewriter-Regular.D3Ib7_Hf.ttf" crossorigin>
		<link rel="modulepreload" href="./_app/immutable/entry/start.DZkK9tV8.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/entry.KadPZDgh.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/scheduler.CHFMnfDQ.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/paths.DDjKdV_q.js">
		<link rel="modulepreload" href="./_app/immutable/entry/app.3qgc3zkF.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/index.BvnlO6mU.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/0.XTtZsPIc.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/each.Cn-hsyDx.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/stores.Dl6dCJxV.js">
		<link rel="modulepreload" href="./_app/immutable/nodes/2.9n6YMzDM.js">
		<link rel="modulepreload" href="./_app/immutable/chunks/rl_excursions.Cxc5-y1v.js"><title>RL Excursions during Pretraining: How early is too early for On-policy Learning?</title><!-- HEAD_svelte-yrkxwy_START --><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z1XRQ6ZG3X" data-svelte-h="svelte-1mc97wv"></script><script data-svelte-h="svelte-woggic">window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());
    gtag("config", "G-Z1XRQ6ZG3X", { send_page_view: false });
  </script><!-- HEAD_svelte-yrkxwy_END --><!-- HEAD_svelte-1u7ey3_START --><meta name="description" content="We study when and how to introduce RL objectives during LLM training: RL on intermediate pretraining checkpoints, sharpening vs expansion, and rollout budgets."><meta property="og:title" content="RL Excursions during Pretraining: How early is too early for On-policy Learning?"><meta property="og:description" content="We study when and how to introduce RL objectives during LLM training: RL on intermediate pretraining checkpoints, sharpening vs expansion, and rollout budgets."><!-- HEAD_svelte-1u7ey3_END -->
  </head>
  <body data-sveltekit-preload-data="hover">
    <div style="display: contents">   <div class="page-upper-right-inline svelte-pbnrg0" data-svelte-h="svelte-6nno4e"><img src="./assets/figures/upper_right_final.png" alt="Institution logos" class="page-upper-right svelte-pbnrg0" loading="lazy" decoding="async"></div> <header class="layout-xl justify-between items-start" data-sveltekit-noscroll data-sveltekit-preload-code="eager"><div class="header-inner mb-8 svelte-1x39zgj"><h1 class="title-font font-bold text-black text-3xl mb-4 leading-tight svelte-1x39zgj">RL Excursions during Pretraining: How early is too early for On-policy Learning?</h1> <div class="meta text-black svelte-1x39zgj"><div class="authors svelte-1x39zgj"><div class="author-line svelte-1x39zgj"><span class="author svelte-1x39zgj"><a href="https://rachitbansal.github.io/" target="_blank" rel="noopener noreferrer" class="author-name author-link svelte-1x39zgj">Rachit Bansal</a>  <sup class="affil-sup svelte-1x39zgj"> <span class="affil-sup-text svelte-1x39zgj" data-svelte-h="svelte-17ywuv4">*</span>  </sup> </span><span class="author svelte-1x39zgj"><a href="https://cmohri.github.io/" target="_blank" rel="noopener noreferrer" class="author-name author-link svelte-1x39zgj">Clara Mohri</a>  <sup class="affil-sup svelte-1x39zgj"> <span class="affil-sup-text svelte-1x39zgj" data-svelte-h="svelte-17ywuv4">*</span>  </sup> </span><span class="author svelte-1x39zgj"><a href="https://sunnytqin.github.io/" target="_blank" rel="noopener noreferrer" class="author-name author-link svelte-1x39zgj">Tian (Sunny) Qin</a>  <sup class="affil-sup svelte-1x39zgj"> <span class="affil-sup-text svelte-1x39zgj" data-svelte-h="svelte-17ywuv4">*</span>  </sup> </span> </div><div class="author-line svelte-1x39zgj"><span class="author svelte-1x39zgj"><a href="https://dmelis.github.io/" target="_blank" rel="noopener noreferrer" class="author-name author-link svelte-1x39zgj">David Alvarez-Melis</a>  <sup class="affil-sup svelte-1x39zgj">  <span class="affil-sup-text svelte-1x39zgj" data-svelte-h="svelte-16hu8i6">†</span> </sup> </span><span class="author svelte-1x39zgj"><a href="https://shamulent.github.io/" target="_blank" rel="noopener noreferrer" class="author-name author-link svelte-1x39zgj">Sham Kakade</a>  <sup class="affil-sup svelte-1x39zgj">  <span class="affil-sup-text svelte-1x39zgj" data-svelte-h="svelte-16hu8i6">†</span> </sup> </span> </div></div> <div class="affiliations svelte-1x39zgj"><div class="affiliation-line svelte-1x39zgj"><span class="affiliation-item svelte-1x39zgj"> <span>* Equal contribution</span></span> <span class="affiliation-sep svelte-1x39zgj" data-svelte-h="svelte-um1ywq"></span><span class="affiliation-item svelte-1x39zgj"> <span>† Equal advising</span></span> </div> <div class="correspondence-line svelte-1x39zgj">Correspondence: {rachitbansal, tqin, cmohri}@g.harvard.edu</div></div> <div class="date svelte-1x39zgj"> </div></div></div> </header> <main> <div> <nav class="toc svelte-wsnayn" aria-hidden="true"> </nav> <div class="layout-xl text-base space-y-12"><div class="md-shell svelte-puphzr"><div class="md-grid svelte-puphzr"><div class="md-output space-y-6 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p><img src="./assets/figures/figure_1.gif" alt="We analyze the effect of RL across intermediate pretraining checkpoints" id="fig-we-analyze-the-effect-of-rl-across-intermediate-pretraining-checkpoints" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="1"><p><strong>Figure 1.</strong> We analyze the effect of RL across intermediate pretraining checkpoints <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">M</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{M}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> and across two settings: RL directly on the base model (<strong>RL Only</strong>; <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mtext>RL</mtext></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{RL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">RL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span>), and RL after SFT (<strong>Standard Pipeline</strong>; <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mrow><mtext>SFT</mtext><mo>→</mo><mtext>RL</mtext></mrow></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{SFT}\rightarrow\text{RL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">SFT</span></span><span class="mrel mtight">→</span><span class="mord text mtight"><span class="mord mtight">RL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span>). We observe: (1) On-policy learning is effective starting very early during standard pretraining. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mtext>RL</mtext></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{RL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">RL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span> models show significant improvement in both <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">pass@1</mtext></mrow><annotation encoding="application/x-tex">\texttt{pass@1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.2222em;"></span><span class="mord text"><span class="mord texttt">pass@1</span></span></span></span></span></span> and <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">pass@k</mtext></mrow><annotation encoding="application/x-tex">\texttt{pass@k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.2222em;"></span><span class="mord text"><span class="mord texttt">pass@k</span></span></span></span></span></span> metrics as soon as <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mtext>K</mtext></mrow><annotation encoding="application/x-tex">2\text{K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">2</span><span class="mord text"><span class="mord">K</span></span></span></span></span></span> steps (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><mn>4</mn><mtext>B</mtext></mrow><annotation encoding="application/x-tex">\sim 4\text{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">4</span><span class="mord text"><span class="mord">B</span></span></span></span></span></span> tokens) of pretraining. (2) In line with prior work, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mrow><mtext>SFT</mtext><mo>→</mo><mtext>RL</mtext></mrow></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{SFT}\rightarrow\text{RL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">SFT</span></span><span class="mrel mtight">→</span><span class="mord text mtight"><span class="mord mtight">RL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span> improves pass@1 performance over <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mtext>SFT</mtext></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{SFT}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">SFT</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span>, but harms <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">pass@32</mtext></mrow><annotation encoding="application/x-tex">\texttt{pass@32}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.2222em;"></span><span class="mord text"><span class="mord texttt">pass@32</span></span></span></span></span></span> suggesting sharpening. (3) In contrast, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mtext>RL</mtext></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{RL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">RL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span> consistently leads to an increase in <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">pass@32</mtext></mrow><annotation encoding="application/x-tex">\texttt{pass@32}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.2222em;"></span><span class="mord text"><span class="mord texttt">pass@32</span></span></span></span></span></span> performance suggesting that RL can actually expand the model distribution to learn new capabilities.</p>
</div></p>
<!-- ## TL;DR

Modern LLM training usually looks like this:

> **Pretraining** → **Supervised fine-tuning (SFT)** → **Reinforcement Learning (RL) via verfiable rewards**

where, pretraining and SFT employ a next-token prediction (NTP) objective on a static external dataset ("off-policy"). While, RL employs a policy optimization objective on the LLM generations ("on-policy").

The use of two distinct training objectives raises a basic but underexplored question
> **At what point during training does an LLM become capable of learning from its own generations (i.e., on-policy)?**
 -->

<!-- ## Rethinking the Training Pipeline -->

<p>As of February 2026, Large Language Model (LLM) training follows a standard pipeline: <strong>pretraining</strong> <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span></span> <strong>supervised fine-tuning</strong> (<strong>SFT</strong>) <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span></span> <strong>reinforcement learning</strong> (<strong>RL</strong>) via verifiable rewards<sup class="footnote-ref"><a href="#fn-ouyang2022" data-fn="ouyang2022">1</a></sup>. These stages contrast in their objectives: Pretraining and SFT employ a Next-Token Prediction (NTP) objective on a static external dataset (&quot;off-policy&quot;). Whereas RL employs a policy optimization objective on the model&#39;s own generations (&quot;on-policy&quot;).</p>
<p>The use of two distinct training objectives raises several interesting but underexplored questions. In this work we systematically investigate this transition between off-policy and on-policy training objectives, asking: </p>
<blockquote class="inline-block bg-neutral-50 border-l-4 border-neutral-600 rounded px-3 py-2 align-middle my-2"><p><strong>How and when should an RL objective be used in LLM training?</strong></p>
</blockquote><p>Furthermore, there has been a recent growing interest in applying RL earlier in training<sup class="footnote-ref"><a href="#fn-arxiv-org-2510-01265" data-fn="arxiv-org-2510-01265">2</a></sup> <sup class="footnote-ref"><a href="#fn-arxiv-org-2509-19249" data-fn="arxiv-org-2509-19249">3</a></sup> <sup class="footnote-ref"><a href="#fn-arxiv-org-2512-03442" data-fn="arxiv-org-2512-03442">4</a></sup>. As a precursor, we ask concretely: <em>at what point during pretraining does the model&#39;s self-generated data become good enough that on-policy learning actually yields meaningful gradient signals?</em></p>
<p>To answer these questions, we perform a rigorous case study of on-policy learning with a focus on LLM reasoning capabilities.
We pretrain an LLM from scratch on a high-quality, reasoning heavy corpus, and sample several intermediate pretraining checkpoints. We perform RL on the base pretraining checkpoints and study these models in comparison with (i) SFT on the base checkpoints, and (ii) the standard SFT <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span></span> RL pipeline.
For all our experiments, we use math reasoning as a testbed since it provides a clean setting with unambiguous and verifiable rewards.</p>
<!-- , and outcome-based RL methods like GRPO are known to work well (at least in post-training). But we're hoping the lessons we learn here generalize to other RL-training scenarios.  -->
<p>In a nutshell, we derive the following insights:</p>
<ul>
<li><strong>Models start to learn from their own generations very early in training</strong>. That is, RL is effective surprisingly early in pretraining. Training with RL significantly improves performance across datasets and metrics prior to pretraining on a large number of tokens.</li>
<li><strong>RL can lead to expansion of the output distribution.</strong> Contrary to recent findings that RL only sharpens the output distribution, we find that early stage RL considerably improves pass@k performance, indicating that &quot;expansion&quot;. We find that the sharpening vs. expansion effect with RL depends on the training pipelines.</li>
<li><strong>Effect of number of rollouts at different stages of model training.</strong> Early pretraining checkpoints might yield sparse or noisy reward. We observe that a larger number of rollouts provides diminishing returns with compute and fewer rollouts could in fact be
more FLOP-efficient.</li>
</ul>
<p>Together, our findings demonstrate the feasibility of
applying RL objectives to what would typically be considered “under-trained” models suggesting that early-stage RL objectives may be effective in improving downstream performance.</p>
<hr>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h2 svelte-puphzr" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-puphzr"><!-- HTML_TAG_START --><h2 id="experimental-setup">Experimental Setup</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="pretraining-checkpoints">Pretraining checkpoints</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>We pretrain a <strong>1B-parameter</strong> decoder-only model (OLMo2 architecture<sup class="footnote-ref"><a href="#fn-arxiv-org-2501-00656" data-fn="arxiv-org-2501-00656">5</a></sup>) from scratch on <strong>50B tokens</strong> of a high-quality mixture (DOLMino, from OLMo2), saving intermediate checkpoints throughout. We then take these checkpoints and run different &quot;post-training&quot; pipelines <em>from each checkpoint</em>.</p>
<details>
<summary>Pretraining details</summary>

<ul>
<li><strong>Architecture:</strong> OLMo2 1B</li>
<li><strong>Tokens:</strong> 50B total (≈ 2.5× Chinchilla-optimal<sup class="footnote-ref"><a href="#fn-arxiv-org-2203-15556" data-fn="arxiv-org-2203-15556">6</a></sup> token count for this model size)</li>
<li><strong>Optimizer:</strong> AdamW with cosine LR decay, peak LR 4e-4</li>
<li><strong>Seq length:</strong> 4096</li>
<li><strong>Batch size:</strong> 512</li>
<li><strong>Data mixture (DOLMino high-quality):</strong> Wikipedia, high-quality web, ~20% math, plus code/reasoning sources</li>
</ul>
</details>

<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="three-training-pipelines">Three training pipelines</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>Let <strong>M<sub>t</sub></strong> be the base checkpoint after <em>t</em> pretraining steps/tokens. We compare three distinct training pipelines:</p>
<ol>
<li><p><strong>RL only:</strong> M<sub>t</sub> → M<sub>t</sub><sup>RL</sup>
We run RL (GRPO) directly on the base checkpoint.</p>
</li>
<li><p><strong>SFT only:</strong> M<sub>t</sub> → M<sub>t</sub><sup>SFT</sup>
We train on ground-truth solutions (teacher-written reasoning traces) using the NTP objective. We use the <em>same questions</em> as in RL, but here the model learns from expert demonstrations.</p>
</li>
<li><p><strong>Standard pipeline:</strong> M<sub>t</sub> → M<sub>t</sub><sup>SFT</sup> → M<sub>t</sub><sup>SFT→RL</sup>
Taking SFT from above, we then apply RL. This is the typical modern recipe and our gold-standard baseline.</p>
</li>
</ol>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="data-and-evaluation">Data and evaluation</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p><strong>Training data:</strong> For both RL and SFT, we use <a href="https://huggingface.co/datasets/nvidia/OpenMathInstruct-1" class="link" target="_blank" rel="external noopener noreferrer">OpenMathInstruct</a><sup class="footnote-ref"><a href="#fn-toshniwal2024" data-fn="toshniwal2024">7</a></sup>—a dataset of math questions with multiple ground-truth solutions per question.</p>
<p><strong>Benchmarks:</strong> We evaluate on GSM8K<sup class="footnote-ref"><a href="#fn-arxiv-org-2110-14168" data-fn="arxiv-org-2110-14168">8</a></sup> (grade-school math) and MATH<sup class="footnote-ref"><a href="#fn-hendrycks2021" data-fn="hendrycks2021">9</a></sup> (competition-level problems).</p>
<p><strong>Metrics:</strong> We report pass@k for k ∈ {1, 8, 32} at temperature T = 0.6.</p>
<blockquote class="inline-block bg-neutral-50 border-l-4 border-neutral-600 rounded px-3 py-2 align-middle my-2"><p><strong>What is pass@k?</strong> pass@1 measures how often the model gets the right answer on its first try. pass@k (for k &gt; 1) measures whether <em>any</em> of k sampled solutions is correct, telling us about the upperbound on model&#39;s reasoning capabilities.</p>
</blockquote><details>
<summary>Details on OpenMathInstruct</summary>

<p>OpenMathInstruct consists of math questions with multiple ground-truth solutions per question. In <strong>SFT</strong>, we train on the provided solutions from the dataset. In <strong>RL</strong>, the model generates its own solutions and receives reward based on whether the final answer is correct.</p>
<p>The dataset contains two main categories:</p>
<ul>
<li><strong>Majority:</strong> Questions inspired by the MATH dataset—challenging competition-level problems</li>
<li><strong>Minority:</strong> Questions inspired by GSM8K—grade-school level math problems</li>
</ul>
</details>

<details>
<summary>Note on evaluating base checkpoints</summary>

<p>Pretraining checkpoints don&#39;t reliably follow instruction formatting, so we need to evaluate them differently. We care about the model&#39;s <em>reasoning ability</em>, not its instruction-following ability.</p>
<ul>
<li><strong>Base checkpoints (M<sub>t</sub>):</strong> Evaluated with <strong>8-shot</strong> prompting (few-shot examples teach the format)</li>
<li><strong>All trained models (SFT/RL):</strong> Evaluated <strong>0-shot</strong> (they learn the format during training)</li>
</ul>
</details>

<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-puphzr" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-puphzr"><!-- HTML_TAG_START --><h2 id="result-1-rl-works-surprisingly-early">Result 1: RL works surprisingly early.</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>Let&#39;s look at what happens when we run RL directly on early pretraining checkpoints.</p>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="direct-rl-competes-with-the-gold-standard-pipeline-on-gsm8k">Direct-RL competes with the gold standard pipeline on GSM8K.</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p><img src="./assets/figures/gsm_passatk_comparison.png" alt="GSM8K results across checkpoints" id="fig-gsm8k-results-across-checkpoints" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="2"><p><strong>Figure 2.</strong> GSM8K results across checkpoints. RL-only improves early and can match SFT→RL after enough pretraining.</p>
</div></p>
<p>We are seeing very promising results on GSM8K. As early as <strong>4B pretraining tokens</strong>, running RL gives us meaningful improvements. For example, pass@1 accuracy jumps from ~2% (base checkpoin, M<sub>t</sub>) to ~18% (after RL, M<sub>t</sub><sup>RL</sup>).  What makes this especially interesting is that 4B tokens is <em>before</em> we&#39;ve even hit the Chinchilla-optimal<sup class="footnote-ref"><a href="#fn-arxiv-org-2203-15556" data-fn="arxiv-org-2203-15556">6</a></sup> token count (i.e., 20B) for this model size. In other words, RL is helping even when the model is still pretty &quot;under-trained&quot; by conventional standards.</p>
<p><strong>More importantly, RL-only competes with the standard pipeline.</strong> By the time we&#39;ve pretrained on 10B+ tokens, the RL-only model actually <em>outperforms</em> the SFT-only model on pass@1, and performs on par with the full SFT→RL pipeline (M<sub>t</sub><sup>SFT→RL</sup>, the gold-standard baseline).</p>
<p>We are quite surprised by this results because the RL-only model M<sub>t</sub><sup>RL</sup> never trains on ground-truth reasoning traces. It only sees its own generated solutions, and a reward signal for whether the final answer is correct. Yet it matches or outperforms the performance of models that explicitly train on expert-written solutions. This suggests that <strong>ground-truth solution traces may not be strictly necessary</strong> to unlock certain reasoning behaviors. A pretraining model can happily bootstrap its way there from self-generated attempts.</p>
<p>We also see significant improvements in pass@k for k=8 and k=32, which we&#39;ll dig into more in the next section (add link here).</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="limitations-on-math">Limitations on MATH.</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p><img src="./assets/figures/math_passatk_comparison.png" alt="MATH results across checkpoints" id="fig-math-results-across-checkpoints" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="3"><p><strong>Figure 3.</strong> MATH results. RL-only improves over the base checkpoint but doesn&#39;t catch up to SFT or SFT→RL on this harder distribution.</p>
</div></p>
<p>The story on MATH is more nuanced. We still consistently see 5-10% improvements in pass@1, pass@8, and pass@32 over the base checkpoints. 
But on MATH, RL-only (M<sub>t</sub><sup>RL</sup>) never quite catches up to SFT or the standard SFT→RL pipeline (M<sub>t</sub><sup>SFT→RL</sup>). The gap persists even as we continue pretraining. MATH problems are significantly harder than GSM8K (competition-level vs. grade-school), and it seems like training purely on on-policy data from early checkpoints has its limits. The model&#39;s self-generated solutions might not be diverse or correct enough to bootstrap strong reasoning on really challenging problems.</p>
<p>Is this a fundamental limitation of the approach, or could we fix it with more data or a larger model? We are currently investigating this!</p>
<p><strong>Result 1 takeaway:</strong> RL from early checkpoints is effective, but task difficulty matters. For easy problems, it can match the standard pipeline. For harder problems, there&#39;s still a gap.</p>
<hr>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-puphzr" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-puphzr"><!-- HTML_TAG_START --><h2 id="result-2-can-we-settle-the-long-time-rl-debate-sharpening-or-expansion">Result 2: Can we settle the long-time RL debate, sharpening or expansion?</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>One of the heated debates in recent work is what RL actually <em>does</em> to a model&#39;s output distribution. Many works<sup class="footnote-ref"><a href="#fn-qin2025" data-fn="qin2025">10</a></sup><sup class="footnote-ref"><a href="#fn-arxiv-org-2507-14843" data-fn="arxiv-org-2507-14843">11</a></sup> <sup class="footnote-ref"><a href="#fn-yue2025" data-fn="yue2025">12</a></sup> claim that RL only sharpens the distribution without teaching any new reasoning behaviors. </p>
<p>We can think about RL&#39;s effect in two ways:</p>
<ul>
<li><p><strong>Sharpening:</strong> pass@1 improves, but pass@k (for large k) doesn&#39;t improve and sometimes it can even decrease. In other words, the model concentrates probability mass on a smaller set of solutions. It&#39;s getting more confident about specific paths, but not discovering new ones.</p>
</li>
<li><p><strong>Expansion:</strong> Both pass@1 and pass@k improve together. This indicates that the model discovers more correct new successful reasoning paths it didn&#39;t have before.</p>
</li>
</ul>
<p>Recent work has claimed that RL mostly just <em>sharpens</em> the distribution without giving the model genuinely new reasoning capabilities. But we found that <strong>whether RL has a sharpening or expansion effect depends on the training pipeline.</strong></p>
<p><img src="./assets/figures/gsm8k_rl_train_dynamics_comparison.png" alt="Training dynamics: sharpening vs expansion" id="fig-training-dynamics-sharpening-vs-expansion" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="4"><p><strong>Figure 4.</strong> Training dynamics. Left: SFT→RL shows sharpening (pass@1 up, pass@32 down during RL). Right: RL-only shows expansion (both pass@1 and pass@32 up).</p>
</div></p>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="standard-pipeline-sft-rl-tends-to-sharpen">Standard pipeline (SFT→RL) tends to sharpen</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>When RL comes <em>after</em> SFT, we reproduce the sharpening effect that others have observed that pass@1 continues to improve during RL while pass@32 actually decreases slightly during RL (after increasing during SFT).
<strong>We hypothesize that</strong> during SFT, the model has already seen ground-truth solutions for these exact questions. So when RL kicks in, it&#39;s mostly refining and concentrating around the reasoning paths it learned during SFT, rather than discovering new ones.</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="rl-only-tends-to-expand">RL-only tends to expand</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>When we run RL directly on the base checkpoint (skipping SFT entirely), we instead observe the expansion effect where <strong>both pass@1 and pass@32 improve</strong>. Without prior exposure to ground-truth solutions, the model appears to explore and discover new reasoning paths through on-policy learning.</p>
<details>
<summary><strong>An important detour: brittleness on early checkpoints</strong></summary>

<p><img src="./assets/figures/gsm8k_seed_rewards.png" alt="Seed brittleness at early checkpoints" id="fig-seed-brittleness-at-early-checkpoints" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm'><p>Figure A1. Seed brittleness at early checkpoints: training reward can look similar while test performance diverges sharply.</p>
</div></p>
<p>Despite these promising results, we also noticed that directly running RL on early checkpoints is <strong>unstable</strong>. </p>
<p>Between 4B and 10B pretraining tokens, we found that RL performance is highly sensitive to random seed. Some seeds give us significant improvements on GSM8K; others barely improve over the base checkpoint at all. But interestingly, both the good and bad seeds achieve similar training rewards. It suggests that RL on early checkpoints can sometimes lead to superficial pattern learning or memorization during RL, rather than genuine reasoning development. The model might be &quot;gaming&quot; the reward signal in ways that don&#39;t transfer to actual problem-solving ability. This is a real limitation we&#39;re still trying to understand. </p>
<p>For earlier checkpoints in our main results, we ran RL across 4 different seeds and reported the best-performing one. </p>
</details>

<p><strong>Result 2 takeaway:</strong> RL&#39;s effect isn&#39;t fixed. Whether you see sharpening or expansion depends on what the model has already learned and how much room it has to explore.</p>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-puphzr" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-puphzr"><!-- HTML_TAG_START --><h2 id="result-3-how-many-rollouts-do-you-actually-need">Result 3: How Many Rollouts Do You Actually Need?</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p><img src="./assets/figures/gsm8k_rollouts_p1-2.png" alt="Rollout scaling trade-offs" id="fig-rollout-scaling-trade-offs" class="block mx-auto unselectable" />
<img src="./assets/figures/gsm8k_rollouts_p8-2.png" alt="pass@1 and pass@8 for different rollout counts" id="fig-pass-1-and-pass-8-for-different-rollout-counts" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="6"><p><strong>Figure 6.</strong> Rollout scaling trade-offs. pass@1 and pass@8 results for different rollout counts on GSM8K-Easy and GSM8K-Hard splits, shown as a function of both training examples and FLOPs. More rollouts improves sample efficiency, but fewer rollouts can be more FLOP-efficient—especially on the hard split.</p>
</div></p>
<p>When we ran RL on early pretraining checkpoints, we ran into a pretty practical problem: the model is pretty bad at the training questions. So we had to deal with the <strong>sparse rewards</strong> problem: most of the model&#39;s attempts are wrong, so RL doesn&#39;t get much useful learning signal from its rollouts.</p>
<p>We had a very natural idea: what if we just sample <em>more</em> rollouts per question? If the model only gets 1 out of 10 attempts right, maybe sampling 64 attempts instead of 5 will give us enough correct solutions to learn from.</p>
<p>However, more rollouts also means more compute per training step. So we wanted to understand <strong>when taking compute into consideration, whether increasing rollouts improve RL training.</strong> </p>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="experimental-setup-1">Experimental setup</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>To study this properly, we simulated &quot;easy&quot; and &quot;hard&quot; training scenarios by splitting our <em>training</em> dataset based on how well the base model does on each question. Concurrent work<sup class="footnote-ref"><a href="#fn-cheng2026isocompute" data-fn="cheng2026isocompute">13</a></sup> performed analysis for number of rollouts using a similar setup. We design two subsets from OpenMathInstruct based on problem difficulty:</p>
<details>
<summary>About OpenMathInstruct structure</summary>

<p>OpenMathInstruct contains two main categories of questions: the majority are inspired by the MATH dataset, which consists of challenging competition-level math problems, while a minority are inspired by the GSM8K dataset, which consists of grade-school level math problems.</p>
</details>

<p>From the training set, we only consider GSM8k-like questions and partition them into two sets:</p>
<ul>
<li><strong>GSM8K-Easy:</strong> Questions where the base model gets 16-64 correct solutions out of 64 attempts (it&#39;s doing okay)</li>
<li><strong>GSM8K-Hard:</strong> Questions where the base model gets ≤8 correct solutions out of 64 attempts (it&#39;s struggling)</li>
</ul>
<p>We then trained with GRPO using either <strong>n=5 rollouts</strong> or <strong>n=64 rollouts</strong> per question, and tracked performance as a function of both:</p>
<ol>
<li><strong>Training examples seen</strong> (sample efficiency)</li>
<li><strong>FLOPs consumed</strong> (compute efficiency)</li>
</ol>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="what-we-found">What we found</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>The results reveal a clear <strong>sample efficiency vs. compute efficiency trade-off</strong>:</p>
<p><strong>Sample efficiency (examples seen):</strong><br>With n=64 rollouts, models converge faster in terms of training steps. You&#39;re squeezing more learning signal out of each question, so you need fewer examples to reach good performance.</p>
<p><strong>Compute efficiency (FLOPs):</strong><br>With n=5 rollouts, training is way more FLOP-efficient, especially early in training. You reach similar performance levels with a fraction of the compute budget.
As training continues (toward 10⁶ FLOPs), the gap narrows. Eventually n=64 catches up or even slightly surpasses n=5. But in the early stages, <em>fewer rollouts win on compute</em>.</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="three-key-takeaways-on-rollouts">Three key takeaways on rollouts</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p><strong>1. Final performance doesn&#39;t depend much on rollout count.</strong> 
Both n=5 and n=64 converge to similar pass@k peaks. You&#39;re not missing out on capability by using fewer rollouts.</p>
<p><strong>2. Clear trade-off between sample and compute efficiency.</strong></p>
<ul>
<li>More rollouts (n=64) gives better sample efficiency, meaning faster convergence per training step.</li>
<li>Fewer rollouts (n=5) gives better compute efficiency, meaning similar performance with less compute.</li>
</ul>
<p><strong>3. The compute advantage is especially pronounced on hard problems.</strong>
On GSM8K-Hard (where rewards are sparse), using n=5 rollouts significantly outperforms n=64 in terms of FLOP efficiency.</p>
<p><strong>Result 3 takeaway:</strong> If you&#39;re training RL with sparse rewards, <strong>fewer rollouts can actually be more efficient</strong><sup class="footnote-ref"><a href="#fn-compute-optimal-rl-llm-scaling-github-io" data-fn="compute-optimal-rl-llm-scaling-github-io">14</a></sup>. You don&#39;t need massive rollout scaling to get good performance.</p>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-puphzr" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-puphzr"><!-- HTML_TAG_START --><h2 id="what-s-next">What&#39;s Next?</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>This study is very much ongoing—we see it as a controlled probe into <em>when</em> RL can help, not a complete recipe for replacing the standard pipeline.</p>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="some-important-caveats">Some important caveats</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p><strong>Task and algorithm scope:</strong><br>We intentionally chose RLVR with GRPO<sup class="footnote-ref"><a href="#fn-arxiv-org-2307-04964" data-fn="arxiv-org-2307-04964">15</a></sup> and focused on math reasoning. It&#39;s a clean setup to study the problem, but by no means comprehensive. Different RL algorithms or tasks (e.g., coding, general reasoning, instruction following) might behave quite differently.</p>
<p><strong>Data mixture matters:</strong><br>Our base model was pretrained on a corpus with substantial math (20%) and reasoning-related content (30%). &quot;RL readiness&quot; likely depends heavily on what&#39;s in the pretraining mix—a model trained mostly on web text might show different dynamics.</p>
<p><strong>Model scale:</strong><br>All our results are from a 1B model. Larger models may show different transitions—maybe they become &quot;RL-ready&quot; earlier, or maybe the brittleness we observed goes away. We don&#39;t know yet.</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-puphzr" data-h3fold="1" open><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-puphzr"><!-- HTML_TAG_START --><h3 id="open-directions-we-re-excited-about">Open directions we&#39;re excited about</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p><strong>Mixing RL into pretraining:</strong><br>Our analysis suggests RL can be effective surprisingly early in training. This raises a natural question: what if we don&#39;t wait for pretraining to finish, but instead <em>interleave</em> RL with the standard next-token prediction objective during pretraining itself?</p>
<p>Recent work has started exploring &quot;RL pretraining&quot;<sup class="footnote-ref"><a href="#fn-arxiv-org-2506-08007" data-fn="arxiv-org-2506-08007">16</a></sup> <sup class="footnote-ref"><a href="#fn-arxiv-org-2510-01265" data-fn="arxiv-org-2510-01265">2</a></sup> <sup class="footnote-ref"><a href="#fn-arxiv-org-2509-19249" data-fn="arxiv-org-2509-19249">3</a></sup>, but there are tons of open questions: How should you schedule the two objectives? What fraction of compute should go to each? Does the optimal data mixture change if you&#39;re doing both objectives at once?</p>
<p><strong>Data mixtures and the expansion vs. sharpening effect:</strong><br>We found that pretraining on lots of math makes RL effective quickly. But we also found that RL after SFT tends to sharpen rather than expand. This suggests an interesting hypothesis: <strong>the effect of RL depends heavily on what the model has already seen.</strong></p>
<p>If we combine NTP and RL objectives during pretraining, maybe the optimal data mixture is different from what&#39;s currently standard. The common paradigm is to pretrain on general web data and save task-specific data for later. But if we&#39;re doing RL from the start, maybe we want more structured reasoning content earlier? Or maybe we want to ensure diversity in problem types to encourage expansion?</p>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-puphzr" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-puphzr"><!-- HTML_TAG_START --><h2 id="appendix">Appendix</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>We include some additional plots and ablations here. </p>
<!-- These are useful sanity-check training dynamics and evaluation choices. -->

<details>
<summary><strong>Training convergence across checkpoints </strong></summary>
    
<p>In this work, we are interested in understanding, given sufficient compute, how well each method performs. Therefore, we train all our RL and SFT runs until convergence. In the two plots below, we confirm that both our SFT and RL runs have been trained until convergence. </p>
<figure>
  <img src="./assets/figures/gsm8k_rl_train_dynamics.png" alt="RL train/val reward and GSM8K pass@1 over RL steps for multiple pretraining checkpoints." width="100%"/>
  <figcaption><strong>Figure A2.</strong> RL reward curves (train/val) and GSM8K pass@1 over RL steps show convergence across checkpoints.</figcaption>
</figure>

    
<figure>
  <img src="./assets/figures/gsm8k_sft_epoch_comparison.png" alt="SFT epoch comparison (5 vs 10 epochs) showing convergence across checkpoints on GSM8K pass@k." width="100%"/>
  <figcaption><strong>Figure A3.</strong> SFT epoch ablation indicates performance converges by ~5 epochs.</figcaption>
</figure>
</details>


<details>
<summary><strong>How we evaluate base checkpoints </strong></summary>
The pretraining checkpoints do not have instruct following capabilities. Our goal is to evaluate their math capabilities, so we use 8-shot in-context examples to prompt the model to answer questions in the correct format. 
<figure>
  <img src="./assets/figures/gsm8k_base_eval_shots.png" alt="n-shot prompting ablation (0/1/8-shot) for evaluating base checkpoints on GSM8K and MATH pass@k." width="100%"/>
  <figcaption><strong>Figure A4.</strong> Few-shot prompting ablation for base checkpoints: 8-shot yields the strongest evaluation performance.</figcaption>
</figure>

</details>

<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h2 svelte-puphzr" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-puphzr"><!-- HTML_TAG_START --><h2 id="citation">Citation</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-puphzr"><div class="md-output svelte-puphzr"><!-- HTML_TAG_START --><p>Please cite this work as:</p>
<pre><code class="language-bibtex">@misc{rbcmsq2026rlexcursions,
  author={Rachit Bansal* and Clara Mohri* and Tian (Sunny) Qin* and David Alvarez-Melis and Sham Kakade},
  title={RL Excursions During Pretraining: How Early Is Too Early for On-Policy Learning?},
  howpublished={url{https://rachitbansal.github.io/rl-excursions/}},
  year={2026}
}
</code></pre>
<p><strong>Feedback?</strong><br>These are open questions we&#39;re still thinking about; we&#39;d love to hear your thoughts!</p>
<p>This is a living document. If you have questions, ideas, or want to discuss any of these findings, feel free to reach out!</p>
<!-- HTML_TAG_END --></div></div> </details></div> <aside class="md-footnotes svelte-puphzr" aria-label="Footnotes"><ol class="svelte-puphzr"><li id="fn-ouyang2022" title="ouyang2022" class="svelte-puphzr"><span class="fn-label svelte-puphzr">1</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Ouyang et al. (2022). <a href="https://arxiv.org/abs/2203.02155" class="link" target="_blank" rel="external noopener noreferrer">Training Language Models to Follow Instructions with Human Feedback</a>. NeurIPS 2022.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2510-01265" title="arxiv-org-2510-01265" class="svelte-puphzr"><span class="fn-label svelte-puphzr">2</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Hatamizadeh et al. (2025). <a href="https://arxiv.org/abs/2510.01265" class="link" target="_blank" rel="external noopener noreferrer">RLP: Reinforcement as a Pretraining Objective</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2509-19249" title="arxiv-org-2509-19249" class="svelte-puphzr"><span class="fn-label svelte-puphzr">3</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Li et al. (2025). <a href="https://arxiv.org/abs/2509.19249" class="link" target="_blank" rel="external noopener noreferrer">Reinforcement Learning on Pre-Training Data</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2512-03442" title="arxiv-org-2512-03442" class="svelte-puphzr"><span class="fn-label svelte-puphzr">4</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Xing et al. (2025). <a href="https://arxiv.org/abs/2512.03442" class="link" target="_blank" rel="external noopener noreferrer">PretrainZero: Reinforcement Active Pretraining</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2501-00656" title="arxiv-org-2501-00656" class="svelte-puphzr"><span class="fn-label svelte-puphzr">5</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Team OLMo (2024). <a href="https://arxiv.org/abs/2501.00656" class="link" target="_blank" rel="external noopener noreferrer">2 OLMo 2 Furious</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2203-15556" title="arxiv-org-2203-15556" class="svelte-puphzr"><span class="fn-label svelte-puphzr">6</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Hoffmann et al. (2022). <a href="https://arxiv.org/abs/2203.15556" class="link" target="_blank" rel="external noopener noreferrer">Training Compute-Optimal Large Language Models</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-toshniwal2024" title="toshniwal2024" class="svelte-puphzr"><span class="fn-label svelte-puphzr">7</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Toshniwal et al. (2024). <a href="https://arxiv.org/abs/2402.10176" class="link" target="_blank" rel="external noopener noreferrer">OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset</a>. NeurIPS 2024.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2110-14168" title="arxiv-org-2110-14168" class="svelte-puphzr"><span class="fn-label svelte-puphzr">8</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Cobbe et al. (2021). <a href="https://arxiv.org/abs/2110.14168" class="link" target="_blank" rel="external noopener noreferrer">Training Verifiers to Solve Math Word Problems</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-hendrycks2021" title="hendrycks2021" class="svelte-puphzr"><span class="fn-label svelte-puphzr">9</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Hendrycks et al. (2021). <a href="https://arxiv.org/abs/2103.03874" class="link" target="_blank" rel="external noopener noreferrer">Measuring Mathematical Problem Solving with the MATH Dataset</a>. NeurIPS 2021.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-qin2025" title="qin2025" class="svelte-puphzr"><span class="fn-label svelte-puphzr">10</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Qin et al. (2025). <a href="https://arxiv.org/abs/2505.22756" class="link" target="_blank" rel="external noopener noreferrer">Decomposing Elements of Problem Solving: What &quot;Math&quot; Does RL Teach?</a>. </p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2507-14843" title="arxiv-org-2507-14843" class="svelte-puphzr"><span class="fn-label svelte-puphzr">11</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Wu et al. (2025). <a href="https://arxiv.org/abs/2507.14843" class="link" target="_blank" rel="external noopener noreferrer">The Invisible Leash: Why RLVR May or May Not Escape Its Origin</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-yue2025" title="yue2025" class="svelte-puphzr"><span class="fn-label svelte-puphzr">12</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Yue et al. (2025). <a href="https://arxiv.org/abs/2504.13837" class="link" target="_blank" rel="external noopener noreferrer">Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-cheng2026isocompute" title="cheng2026isocompute" class="svelte-puphzr"><span class="fn-label svelte-puphzr">13</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Cheng et al. (2026). <a href="https://compute-optimal-rl-llm-scaling.github.io/" class="link" target="_blank" rel="external noopener noreferrer">IsoCompute Playbook: Optimally Scaling Sampling Compute for RL Training of LLMs</a>. </p>
<!-- HTML_TAG_END --></span> </li><li id="fn-compute-optimal-rl-llm-scaling-github-io" title="compute-optimal-rl-llm-scaling-github-io" class="svelte-puphzr"><span class="fn-label svelte-puphzr">14</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Cheng et al. (2026). <a href="https://compute-optimal-rl-llm-scaling.github.io/" class="link" target="_blank" rel="external noopener noreferrer">Isocompute Playbook: Optimally Scaling Sampling Compute for RL Training of LLMs</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2307-04964" title="arxiv-org-2307-04964" class="svelte-puphzr"><span class="fn-label svelte-puphzr">15</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Zheng et al. (2023). <a href="https://arxiv.org/abs/2307.04964" class="link" target="_blank" rel="external noopener noreferrer">Secrets of RLHF in Large Language Models Part I: PPO</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2506-08007" title="arxiv-org-2506-08007" class="svelte-puphzr"><span class="fn-label svelte-puphzr">16</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Dong et al. (2025). <a href="https://arxiv.org/abs/2506.08007" class="link" target="_blank" rel="external noopener noreferrer">Reinforcement Pre-Training</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-biderman2023" title="biderman2023" class="svelte-puphzr"><span class="fn-label svelte-puphzr">17</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Biderman et al. (2023). <a href="https://proceedings.mlr.press/v202/biderman23a.html" class="link" target="_blank" rel="external noopener noreferrer">Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</a>. ICML 2023.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2510-15020" title="arxiv-org-2510-15020" class="svelte-puphzr"><span class="fn-label svelte-puphzr">18</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Chen et al. (2025). <a href="https://arxiv.org/abs/2510.15020" class="link" target="_blank" rel="external noopener noreferrer">The Coverage Principle: How Pre-Training Enables Post-Training</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2107-03374" title="arxiv-org-2107-03374" class="svelte-puphzr"><span class="fn-label svelte-puphzr">19</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Chen et al. (2021). <a href="https://arxiv.org/abs/2107.03374" class="link" target="_blank" rel="external noopener noreferrer">Evaluating Large Language Models Trained on Code</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2310-12773" title="arxiv-org-2310-12773" class="svelte-puphzr"><span class="fn-label svelte-puphzr">20</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Dai et al. (2023). <a href="https://arxiv.org/abs/2310.12773" class="link" target="_blank" rel="external noopener noreferrer">Safe RLHF: Safe Reinforcement Learning from Human Feedback</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2503-07453" title="arxiv-org-2503-07453" class="svelte-puphzr"><span class="fn-label svelte-puphzr">21</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Foster et al. (2025). <a href="https://arxiv.org/abs/2503.07453" class="link" target="_blank" rel="external noopener noreferrer">Is a Good Foundation Necessary for Efficient Reinforcement Learning?</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2501-12948" title="arxiv-org-2501-12948" class="svelte-puphzr"><span class="fn-label svelte-puphzr">22</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Guo et al. (2025). <a href="https://arxiv.org/abs/2501.12948" class="link" target="_blank" rel="external noopener noreferrer">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-li2024" title="li2024" class="svelte-puphzr"><span class="fn-label svelte-puphzr">23</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Li et al. (2024). <a href="https://arxiv.org/abs/2406.11794" class="link" target="_blank" rel="external noopener noreferrer">Datacomp-LM: In Search of the Next Generation of Training Sets for Language Models</a>. NeurIPS 2024.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-1711-05101" title="arxiv-org-1711-05101" class="svelte-puphzr"><span class="fn-label svelte-puphzr">24</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Loshchilov &amp; Hutter (2019). <a href="https://arxiv.org/abs/1711.05101" class="link" target="_blank" rel="external noopener noreferrer">Decoupled Weight Decay Regularization</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2305-18290" title="arxiv-org-2305-18290" class="svelte-puphzr"><span class="fn-label svelte-puphzr">25</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Rafailov et al. (2023). <a href="https://arxiv.org/abs/2305.18290" class="link" target="_blank" rel="external noopener noreferrer">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>. NeurIPS 2023.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2402-03300" title="arxiv-org-2402-03300" class="svelte-puphzr"><span class="fn-label svelte-puphzr">26</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Shao et al. (2024). <a href="https://arxiv.org/abs/2402.03300" class="link" target="_blank" rel="external noopener noreferrer">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2409-19256" title="arxiv-org-2409-19256" class="svelte-puphzr"><span class="fn-label svelte-puphzr">27</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Sheng et al. (2024). <a href="https://arxiv.org/abs/2409.19256" class="link" target="_blank" rel="external noopener noreferrer">HybridFlow: A Flexible and Efficient RLHF Framework</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-wei2022" title="wei2022" class="svelte-puphzr"><span class="fn-label svelte-puphzr">28</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Wei et al. (2022). <a href="https://arxiv.org/abs/2109.01652" class="link" target="_blank" rel="external noopener noreferrer">Finetuned Language Models Are Zero-Shot Learners</a>. ICLR 2022.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-arxiv-org-2512-07783" title="arxiv-org-2512-07783" class="svelte-puphzr"><span class="fn-label svelte-puphzr">29</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Zhang et al. (2025). <a href="https://arxiv.org/abs/2512.07783" class="link" target="_blank" rel="external noopener noreferrer">On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models</a>.</p>
<!-- HTML_TAG_END --></span> </li><li id="fn-zhou2023" title="zhou2023" class="svelte-puphzr"><span class="fn-label svelte-puphzr">30</span> <span class="fn-text svelte-puphzr"><!-- HTML_TAG_START --><p>Zhou et al. (2023). <a href="https://arxiv.org/abs/2305.11206" class="link" target="_blank" rel="external noopener noreferrer">Lima: Less Is More for Alignment</a>. NeurIPS 2023.</p>
<!-- HTML_TAG_END --></span> </li></ol></aside></div> </div></div></div></main>  
			
			<script>
				{
					__sveltekit_i4l3w = {
						base: new URL(".", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("./_app/immutable/entry/start.DZkK9tV8.js"),
						import("./_app/immutable/entry/app.3qgc3zkF.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 2],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
  </body>
</html>
