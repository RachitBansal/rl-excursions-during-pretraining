<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <!-- Tab icon: Harvard logo (must come early; no favicon.ico to avoid override) -->
    <link rel="icon" type="image/svg+xml" href="../assets/figures/harvard.svg" />
    <link rel="shortcut icon" type="image/svg+xml" href="../assets/figures/harvard.svg" />
    <link
      rel="preload"
      href="../fonts/ibm_plex_sans/IBMPlexSans-Regular.ttf"
      as="font"
      type="font/ttf"
      crossorigin
    />
    <link
      rel="preload"
      href="../fonts/newsreader/Newsreader_14pt-Italic.ttf"
      as="font"
      type="font/ttf"
      crossorigin
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-ZSV2YDNY0W"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-ZSV2YDNY0W", { send_page_view: false });
    </script>
    <script>
      if (window.location.hostname === "localhost") {
        window.gtag = () => {};
      }
    </script>
    
		<link href="../_app/immutable/assets/0.ChBBf1Xn.css" rel="stylesheet">
		<link href="../_app/immutable/assets/rl_excursions.CZR1otz5.css" rel="stylesheet">
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_AMS-Regular.BQhdFMY1.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_AMS-Regular.DMm9YOAa.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_AMS-Regular.DRggAlZN.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Caligraphic-Bold.Dq_IR9rO.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Caligraphic-Bold.BEiXGLvX.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Caligraphic-Bold.ATXxdsX0.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Caligraphic-Regular.Di6jR-x-.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Caligraphic-Regular.CTRA-rTL.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Caligraphic-Regular.wX97UBjC.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Fraktur-Bold.CL6g_b3V.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Fraktur-Bold.BsDP51OF.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Fraktur-Bold.BdnERNNW.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Fraktur-Regular.CTYiF6lA.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Fraktur-Regular.Dxdc4cR9.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Fraktur-Regular.CB_wures.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Main-Bold.Cx986IdX.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Main-Bold.Jm3AIy58.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Main-Bold.waoOVXN0.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Main-BoldItalic.DxDJ3AOS.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Main-BoldItalic.SpSLRI95.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Main-BoldItalic.DzxPMmG6.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Main-Italic.NWA7e6Wa.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Main-Italic.BMLOBm91.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Main-Italic.3WenGoN9.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Main-Regular.B22Nviop.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Main-Regular.Dr94JaBh.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Main-Regular.ypZvNtVU.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Math-BoldItalic.CZnvNsCZ.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Math-BoldItalic.iY-2wyZ7.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Math-BoldItalic.B3XSjfu4.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Math-Italic.t53AETM-.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Math-Italic.DA0__PXp.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Math-Italic.flOr_0UB.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_SansSerif-Bold.D1sUS0GD.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_SansSerif-Bold.DbIhKOiC.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_SansSerif-Bold.CFMepnvq.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_SansSerif-Italic.C3H0VqGB.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_SansSerif-Italic.DN2j7dab.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_SansSerif-Italic.YYjJ1zSn.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_SansSerif-Regular.DDBCnlJ7.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_SansSerif-Regular.CS6fqUqJ.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_SansSerif-Regular.BNo7hRIc.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Script-Regular.D3wIWfF6.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Script-Regular.D5yQViql.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Script-Regular.C5JkGWo-.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Size1-Regular.mCD8mA8B.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Size1-Regular.C195tn64.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Size1-Regular.Dbsnue_I.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Size2-Regular.Dy4dx90m.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Size2-Regular.oD1tc_U0.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Size2-Regular.B7gKUWhC.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Size3-Regular.CTq5MqoE.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Size3-Regular.DgpXs0kz.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Size4-Regular.Dl5lxZxV.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Size4-Regular.BF-4gkZK.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Size4-Regular.DWFBv043.ttf" crossorigin>
		<link rel="preload" as="font" type="font/woff2" href="../_app/immutable/assets/KaTeX_Typewriter-Regular.CO6r4hn1.woff2" crossorigin>
		<link rel="preload" as="font" type="font/woff" href="../_app/immutable/assets/KaTeX_Typewriter-Regular.C0xS9mPB.woff" crossorigin>
		<link rel="preload" as="font" type="font/ttf" href="../_app/immutable/assets/KaTeX_Typewriter-Regular.D3Ib7_Hf.ttf" crossorigin>
		<link rel="modulepreload" href="../_app/immutable/entry/start.BE5RMp52.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/entry.IXSR7iWE.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/scheduler.CHFMnfDQ.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/paths.Dz6Zim6s.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.DQ2muWfq.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/index.BvnlO6mU.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.g_BImftG.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/each.Cn-hsyDx.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/stores.Df1n8OY0.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/3.Du5yjrYf.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/rl_excursions.B1k4g1b8.js"><title>RL Excursions during Pre-training</title><!-- HEAD_svelte-yrkxwy_START --><script async src="https://www.googletagmanager.com/gtag/js?id=G-Z1XRQ6ZG3X" data-svelte-h="svelte-1mc97wv"></script><script data-svelte-h="svelte-woggic">window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());
    gtag("config", "G-Z1XRQ6ZG3X", { send_page_view: false });
  </script><!-- HEAD_svelte-yrkxwy_END --><!-- HEAD_svelte-1u7ey3_START --><meta name="description" content="RL Excursions during Pre-training: How early is too early for On-policy Learning?"><meta property="og:title" content="RL Excursions during Pre-training"><meta property="og:description" content="RL Excursions during Pre-training: How early is too early for On-policy Learning?"><!-- HEAD_svelte-1u7ey3_END -->
  </head>
  <body data-sveltekit-preload-data="hover">
    <div style="display: contents">   <div class="page-upper-right-inline svelte-pbnrg0" data-svelte-h="svelte-6nno4e"><img src="../assets/figures/upper_right_final.png" alt="Institution logos" class="page-upper-right svelte-pbnrg0" loading="lazy" decoding="async"></div> <header class="layout-xl justify-between items-start" data-sveltekit-noscroll data-sveltekit-preload-code="eager"><div class="header-inner mb-8 svelte-1j41yyi"><h1 class="title-font font-bold text-black text-3xl mb-4 leading-tight svelte-1j41yyi">RL Excursions during Pre-training: How early is too early for On-policy Learning?</h1> <div class="meta text-black svelte-1j41yyi"><div class="authors svelte-1j41yyi"><span class="author svelte-1j41yyi"><span class="author-name svelte-1j41yyi">Rachit Bansal</span>  <sup class="affil-sup svelte-1j41yyi"> <span class="affil-sup-text svelte-1j41yyi" data-svelte-h="svelte-17ywuv4">*</span> </sup> </span><span class="author svelte-1j41yyi"><span class="author-name svelte-1j41yyi">Clara Mohri</span>  <sup class="affil-sup svelte-1j41yyi"> <span class="affil-sup-text svelte-1j41yyi" data-svelte-h="svelte-17ywuv4">*</span> </sup> </span><span class="author svelte-1j41yyi"><span class="author-name svelte-1j41yyi">Tian (Sunny) Qin</span>  <sup class="affil-sup svelte-1j41yyi"> <span class="affil-sup-text svelte-1j41yyi" data-svelte-h="svelte-17ywuv4">*</span> </sup> </span><span class="author svelte-1j41yyi"><span class="author-name svelte-1j41yyi">David Alvarez-Melis</span>   </span><span class="author svelte-1j41yyi"><span class="author-name svelte-1j41yyi">Sham Kakade</span>   </span></div> <div class="affiliations svelte-1j41yyi"><div class="affiliation-line svelte-1j41yyi"><span class="affiliation-item svelte-1j41yyi"><img src="../assets/figures/harvard.svg" alt="Harvard University" title="Harvard University" class="affil-logo affil-logo--legend svelte-1j41yyi" loading="lazy" decoding="async"> <span>Harvard University</span></span> <span class="affiliation-sep svelte-1j41yyi" data-svelte-h="svelte-djw72">, </span><span class="affiliation-item svelte-1j41yyi"> <span>Kempner Institute at Harvard</span></span> <span class="affiliation-sep svelte-1j41yyi" data-svelte-h="svelte-djw72">, </span><span class="affiliation-item svelte-1j41yyi"> <span>* Equal contribution</span></span> </div></div> <div class="date svelte-1j41yyi"> </div></div></div> </header> <main>   <div> <nav class="toc svelte-wsnayn" aria-hidden="true"> </nav> <div class="layout-xl text-base space-y-12"><div class="md-shell svelte-1cybysc"><div class="md-grid svelte-1cybysc"><div class="md-output space-y-6 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p><img src="../assets/figures/figure_1.png" alt="We analyze the effect of RL across intermediate pretraining checkpoints" id="fig-we-analyze-the-effect-of-rl-across-intermediate-pretraining-checkpoints" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="1"><p><strong>Figure 1.</strong> We analyze the effect of RL across intermediate pretraining checkpoints <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">M</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{M}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> and across two settings: RL directly on the base model (<strong>RL Only</strong>; <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mtext>RL</mtext></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{RL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">RL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span>), and RL after SFT (<strong>Standard Pipeline</strong>; <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mrow><mtext>SFT</mtext><mo>→</mo><mtext>RL</mtext></mrow></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{SFT}\rightarrow\text{RL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">SFT</span></span><span class="mrel mtight">→</span><span class="mord text mtight"><span class="mord mtight">RL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span>). We observe: (1) On-policy learning is effective starting very early during standard pretraining. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mtext>RL</mtext></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{RL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">RL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span> models show significant improvement in both <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">pass@1</mtext></mrow><annotation encoding="application/x-tex">\texttt{pass@1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.2222em;"></span><span class="mord text"><span class="mord texttt">pass@1</span></span></span></span></span></span> and <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">pass@k</mtext></mrow><annotation encoding="application/x-tex">\texttt{pass@k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.2222em;"></span><span class="mord text"><span class="mord texttt">pass@k</span></span></span></span></span></span> metrics as soon as <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mtext>K</mtext></mrow><annotation encoding="application/x-tex">2\text{K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">2</span><span class="mord text"><span class="mord">K</span></span></span></span></span></span> steps (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><mn>4</mn><mtext>B</mtext></mrow><annotation encoding="application/x-tex">\sim 4\text{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">4</span><span class="mord text"><span class="mord">B</span></span></span></span></span></span> tokens) of pretraining. (2) In line with prior work, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mrow><mtext>SFT</mtext><mo>→</mo><mtext>RL</mtext></mrow></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{SFT}\rightarrow\text{RL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">SFT</span></span><span class="mrel mtight">→</span><span class="mord text mtight"><span class="mord mtight">RL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span> improves pass@1 performance over <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mtext>SFT</mtext></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{SFT}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">SFT</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span>, but harms <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">pass@32</mtext></mrow><annotation encoding="application/x-tex">\texttt{pass@32}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.2222em;"></span><span class="mord text"><span class="mord texttt">pass@32</span></span></span></span></span></span> suggesting sharpening. (3) In contrast, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="script">M</mi><mi>t</mi><mtext>RL</mtext></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{M}_t^{\text{RL}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0883em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathcal">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">RL</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span> consistently leads to an increase in <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">pass@32</mtext></mrow><annotation encoding="application/x-tex">\texttt{pass@32}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.2222em;"></span><span class="mord text"><span class="mord texttt">pass@32</span></span></span></span></span></span> performance suggesting that RL can actually expand the model distribution to learn new capabilities.</p>
</div></p>
<!-- ## TL;DR

Modern LLM training usually looks like this:

> **Pretraining** → **Supervised fine-tuning (SFT)** → **Reinforcement Learning (RL) via verfiable rewards**

where, pre-training and SFT employ a next-token prediction (NTP) objective on a static external dataset ("off-policy"). While, RL employs a policy optimization objective on the LLM generations ("on-policy").

The use of two distinct training objectives raises a basic but underexplored question
> **At what point during training does an LLM become capable of learning from its own generations (i.e., on-policy)?**
 -->

<!-- ## Rethinking the Training Pipeline -->

<p>As of February 2026, Large Language Model (LLM) training follows a standard pipeline: <strong>pretraining</strong> <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span></span> <strong>supervised fine-tuning</strong> (<strong>SFT</strong>) <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span></span> <strong>reinforcement learning</strong> (<strong>RL</strong>) via verifiable rewards. These stages contrast in their objectives: Pretraining and SFT employ a Next-Token Prediction (NTP) objective on a static external dataset (&quot;off-policy&quot;). Whereas RL employs a policy optimization objective on the model&#39;s own generations (&quot;on-policy&quot;).</p>
<p>The use of two distinct training objectives raises several interesting but underexplored questions. In this work we systematically investigate this transition between off-policy and on-policy training objectives, asking: </p>
<blockquote class="inline-block bg-neutral-50 border-l-4 border-neutral-600 rounded px-3 py-2 align-middle my-2"><p><strong>How and when should we be using an RL objective during LLM training?</strong></p>
</blockquote><p>Furthermore, there has been a recent growing interest in applying RL earlier in training (Hatamizadeh et al., 2025; Li et al., 2025; Xing
et al., 2025)[reference]. As a precursor, we ask concretely: <em>at what point during pretraining does the model&#39;s self-generated data become good enough that on-policy learning actually yields meaningful gradient signals?</em></p>
<p>To answer these questions, we perform a rigorous case study of on-policy learning with a focus on LLM reasoning capabilities.
We pretrain an LLM from scratch on a high-quality, reasoning heavy corpus, and sample several intermediate pre-training checkpoints. We perform RL on the base pre-training checkpoints and study these models in comparison with (i) SFT on the base checkpoints, and (ii) the standard SFT <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span></span> RL pipeline.
For all our experiments, we use math reasoning as a testbed since it provides a clean setting with unambiguous and verifiable rewards.</p>
<!-- , and outcome-based RL methods like GRPO are known to work well (at least in post-training). But we're hoping the lessons we learn here generalize to other RL-training scenarios.  -->
<p>In a nutshell, we derive the following insights:</p>
<ul>
<li><strong>Models start to learn from their own generations very early in training</strong>. That is, RL is effective surprisingly early in pretraining. Training with RL significantly improves performance across datasets and metrics prior to pretraining on a large number of tokens.</li>
<li><strong>RL can lead to expansion of the output distribution.</strong> Contrary to recent findings that RL only sharpens the output distribution, we find that early stage RL considerably improves pass@k performance, indicating that &quot;expansion&quot;. We find that the sharpening vs. expansion effect with RL depends on the training pipelines.</li>
<li><strong>Effect of number of rollouts at different stages of model training.</strong> Early pre-training checkpoints might yield sparse or noisy reward. We observe that a larger number of rollouts provides diminishing returns with compute and fewer rollouts could in fact be
more FLOP-efficient.</li>
</ul>
<p>Together, our findings demonstrate the feasibility of
applying RL objectives to what would typically be considered “under-trained” models suggesting that early-stage RL objectives may be effective in improving downstream performance.</p>
<hr>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="experimental-setup">Experimental Setup</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="pretraining-checkpoints">Pretraining checkpoints</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>We pretrain a <strong>1B-parameter</strong> decoder-only model (OLMo2 architecture) from scratch on <strong>50B tokens</strong> of a high-quality mixture (DOLMino, from OLMo2), saving intermediate checkpoints throughout. We then take these checkpoints and run different &quot;post-training&quot; pipelines <em>from each checkpoint</em>.</p>
<details>
<summary>Pretraining details</summary>

<ul>
<li><strong>Architecture:</strong> OLMo2 1B</li>
<li><strong>Tokens:</strong> 50B total (≈ 2.5× Chinchilla-optimal token count for this model size)</li>
<li><strong>Optimizer:</strong> AdamW with cosine LR decay, peak LR 4e-4</li>
<li><strong>Seq length:</strong> 4096</li>
<li><strong>Batch size:</strong> 512</li>
<li><strong>Data mixture (DOLMino high-quality):</strong> Wikipedia, high-quality web, ~20% math, plus code/reasoning sources</li>
</ul>
</details>

<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="three-training-pipelines">Three training pipelines</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>Let <strong>M<sub>t</sub></strong> be the base checkpoint after <em>t</em> pretraining steps/tokens. We compare three distinct training pipelines:</p>
<ol>
<li><p><strong>RL only:</strong> M<sub>t</sub> → M<sub>t</sub><sup>RL</sup>
We run RL (GRPO) directly on the base checkpoint.</p>
</li>
<li><p><strong>SFT only:</strong> M<sub>t</sub> → M<sub>t</sub><sup>SFT</sup>
We train on ground-truth solutions (teacher-written reasoning traces) using the NTP objective. We use the <em>same questions</em> as in RL, but here the model learns from expert demonstrations.</p>
</li>
<li><p><strong>Standard pipeline:</strong> M<sub>t</sub> → M<sub>t</sub><sup>SFT</sup> → M<sub>t</sub><sup>SFT→RL</sup>
Taking SFT from above, we then apply RL. This is the typical modern recipe and our gold-standard baseline.</p>
</li>
</ol>
<p>Here&#39;s how the three pipelines compare visually (<strong>SQ: idea is nice but plot looks bad</strong>):</p>
<pre><code class="language-mermaid">flowchart LR
    Mt[&quot;Base checkpointat t tokens&quot;]
    
    Mt --&gt;|&quot;RL&quot;| MRL[&quot;RL-only model&quot;]
    Mt --&gt;|&quot;SFT&quot;| MSFT[&quot;SFT-only model&quot;]
    MSFT --&gt;|&quot;RL&quot;| MSFTRL[&quot;Standard pipeline model&quot;]
    
    style Mt fill:#e1f5ff
    style MRL fill:#fff4e1
    style MSFT fill:#ffe1f5
    style MSFTRL fill:#e1ffe1
</code></pre>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="data-and-evaluation">Data and evaluation</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p><strong>Training data:</strong> For both RL and SFT, we use <a href="https://huggingface.co/datasets/nvidia/OpenMathInstruct-1" class="link" target="_blank" rel="external noopener noreferrer">OpenMathInstruct</a>—a dataset of math questions with multiple ground-truth solutions per question.</p>
<p><strong>Benchmarks:</strong> We evaluate on GSM8K (grade-school math) and MATH (competition-level problems).</p>
<p><strong>Metrics:</strong> We report pass@k for k ∈ {1, 8, 32} at temperature T = 0.6.</p>
<blockquote class="inline-block bg-neutral-50 border-l-4 border-neutral-600 rounded px-3 py-2 align-middle my-2"><p><strong>What is pass@k?</strong> pass@1 measures how often the model gets the right answer on its first try. pass@k (for k &gt; 1) measures whether <em>any</em> of k sampled solutions is correct, telling us about the upperbound on model&#39;s reasoning capabilities.</p>
</blockquote><details>
<summary>Details on OpenMathInstruct</summary>

<p>OpenMathInstruct consists of math questions with multiple ground-truth solutions per question. In <strong>SFT</strong>, we train on the provided solutions from the dataset. In <strong>RL</strong>, the model generates its own solutions and receives reward based on whether the final answer is correct.</p>
<p>The dataset contains two main categories:</p>
<ul>
<li><strong>Majority:</strong> Questions inspired by the MATH dataset—challenging competition-level problems</li>
<li><strong>Minority:</strong> Questions inspired by GSM8K—grade-school level math problems</li>
</ul>
</details>

<details>
<summary>Note on evaluating base checkpoints</summary>

<p>Pretraining checkpoints don&#39;t reliably follow instruction formatting, so we need to evaluate them differently. We care about the model&#39;s <em>reasoning ability</em>, not its instruction-following ability.</p>
<ul>
<li><strong>Base checkpoints (M<sub>t</sub>):</strong> Evaluated with <strong>8-shot</strong> prompting (few-shot examples teach the format)</li>
<li><strong>All trained models (SFT/RL):</strong> Evaluated <strong>0-shot</strong> (they learn the format during training)</li>
</ul>
</details>

<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="result-1-rl-works-surprisingly-early">Result 1: RL works surprisingly early.</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>Let&#39;s look at what happens when we run RL directly on early pretraining checkpoints.</p>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="gsm8k-rl-kicks-in-fast">GSM8K: RL kicks in fast</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p><img src="../assets/figures/gsm_passatk_comparison.png" alt="GSM8K results across checkpoints" id="fig-gsm8k-results-across-checkpoints" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="2"><p><strong>Figure 2.</strong> GSM8K results across checkpoints. RL-only improves early and can match SFT→RL after enough pretraining.</p>
</div></p>
<p>The results on GSM8K are pretty striking. As early as <strong>4B pretraining tokens</strong>, running RL gives us meaningful improvements. For example, pass@1 accuracy jumps from ~2% (base checkpoint) to ~18% (after RL).  What makes this especially interesting is that 4B tokens is <em>before</em> we&#39;ve even hit the Chinchilla-optimal token count for this model size. In other words, RL is helping even when the model is still pretty &quot;undertrained&quot; by conventional standards.</p>
<p><strong>More importantly, RL-only competes with the standard pipeline.</strong> By the time we&#39;ve pretrained on 10B+ tokens, something cool happens: the RL-only model actually <em>outperforms</em> the SFT-only model on pass@1, and performs on par with the full SFT→RL pipeline (our gold-standard baseline).</p>
<p><strong>Why are we surprised?</strong> RL-only never trains on ground-truth reasoning traces. It only sees its own generated solutions, and a reward signal for whether the final answer is correct. Yet it matches or outperforms the performance of models that explicitly train on expert-written solutions. This suggests that <strong>ground-truth solution traces may not be strictly necessary or even optimal</strong> to unlock certain reasoning behaviors. A pretraining model can happily bootstrap its way there from self-generated attempts.</p>
<p>We also see significant improvements in pass@k for k=8 and k=32, which we&#39;ll dig into more in the next section (add link here).</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="math-not-quite-there-yet">MATH: Not quite there yet</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p><img src="../assets/figures/math_passatk_comparison.png" alt="MATH results across checkpoints" id="fig-math-results-across-checkpoints" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="3"><p><strong>Figure 3.</strong> MATH results. RL-only improves over the base checkpoint but doesn&#39;t catch up to SFT or SFT→RL on this harder distribution.</p>
</div></p>
<p>The story on MATH is more nuanced. We still consistently see 5-10% improvements in pass@1, pass@8, and pass@32 over the base checkpoints. 
But on MATH, RL-only never quite catches up to SFT or the standard SFT→RL pipeline. The gap persists even as we continue pretraining. MATH problems are significantly harder than GSM8K (competition-level vs. grade-school), and it seems like training purely on on-policy data from early checkpoints has its limits. The model&#39;s self-generated solutions might not be diverse or correct enough to bootstrap strong reasoning on really challenging problems.</p>
<p>Is this a fundamental limitation of the approach, or could we fix it with more data or a larger model? We are currently investigating this!</p>
<p><strong>Result 1 takeaway:</strong> RL from early checkpoints is effective, but task difficulty matters. For easy problems, it can match the standard pipeline. For harder problems, there&#39;s still a gap.</p>
<hr>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="result-2-can-we-settle-the-long-time-rl-debate-sharpening-or-expansion">Result 2: Can we settle the long-time RL debate, sharpening or expansion?</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>One of the heated debate in recent claims about what RL actually <em>does</em> to a model&#39;s output distribution. Many works [cite] claims that RL only sharpens distribution without teaching any new reasoning behaviors. </p>
<p>We can think about RL&#39;s effect in two ways:</p>
<ul>
<li><p><strong>Sharpening:</strong> pass@1 improves, but pass@k (for large k) doesn&#39;t improve and sometimes it can even decrease. In other words, the model concentrates probability mass on a smaller set of solutions. It&#39;s getting more confident about specific paths, but not discovering new ones.</p>
</li>
<li><p><strong>Expansion:</strong> Both pass@1 and pass@k improve together.This indicates that the model discovers more correct new successful reasoning paths it didn&#39;t have before.</p>
</li>
</ul>
<p>Recent work has claimed that RL mostly just <em>sharpens</em> the distribution without giving the model genuinely new reasoning capabilities. But we found that <strong>whether RL has a sharpening or expansion effect depends on the training pipeline.</strong></p>
<p><img src="../assets/figures/gsm8k_rl_train_dynamics_comparison.png" alt="Training dynamics: sharpening vs expansion" id="fig-training-dynamics-sharpening-vs-expansion" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="4"><p><strong>Figure 4.</strong> Training dynamics. Left: SFT→RL shows sharpening (pass@1 up, pass@32 down during RL). Right: RL-only shows expansion (both pass@1 and pass@32 up).</p>
</div></p>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="standard-pipeline-sft-rl-tends-to-sharpen">Standard pipeline (SFT→RL) tends to sharpen</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>When RL comes <em>after</em> SFT, we reproduce the sharpening effect that others have observed that pass@1 continues to improve during RL while pass@32 actually decreases slightly during RL (after increasing during SFT).
<strong>We hypothesize that</strong> during SFT, the model has already seen ground-truth solutions for these exact questions. So when RL kicks in, it&#39;s mostly refining and concentrating around the reasoning paths it learned during SFT, rather than discovering new ones.</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="rl-only-tends-to-expand">RL-only tends to expand</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>When we run RL directly on the base checkpoint (skipping SFT entirely), we instead observe the expansion effect where <strong>both pass@1 and pass@32 improve</strong>. Without prior exposure to ground-truth solutions, the model appears to explore and discover new reasoning paths through on-policy learning.</p>
<details>
<summary><strong>An important detour: brittleness on early checkpoints</strong></summary>
![Seed brittleness at early checkpoints](/assets/figures/gsm8k_seed_rewards.png "Figure 7. Seed brittleness at early checkpoints: training reward can look similar while test performance diverges sharply.")


<p>Despite these promising results, we need to be honest about something: direct RL training on early checkpoints is <strong>unstable</strong>. </p>
<p>Between 4B and 10B pretraining tokens, we found that RL performance is highly sensitive to random seed. Some seeds give us significant improvements on GSM8K; others barely improve over the base checkpoint at all—even though they achieve similar training rewards.</p>
<p>What does this mean? It suggests that RL on early checkpoints can sometimes lead to superficial pattern learning or memorization rather than genuine reasoning development. The model might be &quot;gaming&quot; the reward signal in ways that don&#39;t transfer to actual problem-solving ability.</p>
<p>This is a real limitation we&#39;re still trying to understand. It&#39;s one reason why, for earlier checkpoints in our main results, we ran RL across 4 different seeds and reported the best-performing one. Not ideal, but it gives us a sense of what&#39;s possible when things go well.</p>
</details>

<p><strong>Result 2 takeaway:</strong> RL&#39;s effect isn&#39;t fixed. Whether you see sharpening or expansion depends on what the model has already learned and how much room it has to explore.</p>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="result-3-how-many-rollouts-do-you-actually-need">Result 3: How Many Rollouts Do You Actually Need?</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p><img src="../assets/figures/gsm8k_rollouts_p8-2.png" alt="Rollout scaling trade-offs" id="fig-rollout-scaling-trade-offs" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="5"><p><strong>Figure 5.</strong> Rollout scaling trade-offs. More rollouts improves sample efficiency, but fewer rollouts can be more FLOP-efficient—especially on the hard split.</p>
</div></p>
<p>When we ran RL on early pretraining checkpoints, we ran into a pretty practical problem: the model is pretty bad at the training questions. So we had to deal with the <strong>sparse rewards</strong> problem: most of the model&#39;s attempts are wrong, so RL doesn&#39;t get much useful learning signal from its rollouts.</p>
<p>We had a very natural idea: what if we just sample <em>more</em> rollouts per question? If the model only gets 1 out of 10 attempts right, maybe sampling 64 attempts instead of 5 will give us enough correct solutions to learn from.</p>
<p>However, more rollouts also means more compute per training step. So we wanted to understand <strong>when taking compute into consideration, wether increasing rollouts improve RL training.</strong> </p>
<!-- HTML_TAG_END --></div><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="experimental-setup-1">Experimental setup</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>To study this properly, we simulated &quot;easy&quot; and &quot;hard&quot; training scenarios by splitting our <em>training</em> dataset based on how well the base model does on each question. We designed two subsets from OpenMathInstruct:</p>
<details>
<summary>About OpenMathInstruct structure</summary>

<p>OpenMathInstruct contains two main categories of questions: the majority are inspired by the MATH dataset, which consists of challenging competition-level math problems, while a minority are inspired by the GSM8K dataset, which consists of grade-school level math problems.</p>
</details>

<p>From the training set, we only consider GSM-like questions and partition them into two sets:</p>
<ul>
<li><strong>GSM8K-Easy:</strong> Questions where the base model gets 16-64 correct solutions out of 64 attempts (it&#39;s doing okay)</li>
<li><strong>GSM8K-Hard:</strong> Questions where the base model gets ≤8 correct solutions out of 64 attempts (it&#39;s struggling)</li>
</ul>
<p>We then trained with GRPO using either <strong>n=5 rollouts</strong> or <strong>n=64 rollouts</strong> per question, and tracked performance as a function of both:</p>
<ol>
<li><strong>Training examples seen</strong> (sample efficiency)</li>
<li><strong>FLOPs consumed</strong> (compute efficiency)</li>
</ol>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="what-we-found">What we found</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p><img src="../assets/figures/gsm8k_rollouts_p1-2.png" alt="pass@1 and pass@8 for different rollout counts" id="fig-pass-1-and-pass-8-for-different-rollout-counts" class="block mx-auto unselectable" /><div class='md-figcaption text-left text-gray-500 mb-4 md:px-8 lg:px-12 text-sm' data-fig-num="6"><p><strong>Figure 6.</strong> pass@1 and pass@8 results for different rollout counts on GSM8K-Easy and GSM8K-Hard splits, shown as a function of both training examples and FLOPs.</p>
</div></p>
<p>The results reveal a clear <strong>sample efficiency vs. compute efficiency trade-off</strong>:</p>
<p><strong>Sample efficiency (examples seen):</strong><br>With n=64 rollouts, models converge faster in terms of training steps. You&#39;re squeezing more learning signal out of each question, so you need fewer examples to reach good performance.</p>
<p><strong>Compute efficiency (FLOPs):</strong><br>But here&#39;s the twist—n=5 rollouts is way more FLOP-efficient, especially early in training. You reach similar performance levels with a fraction of the compute budget.</p>
<p>As training continues (toward 10⁶ FLOPs), the gap narrows. Eventually n=64 catches up or even slightly surpasses n=5. But in the early stages, fewer rollouts win on compute.</p>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h3 svelte-1cybysc" data-h3fold="1" ><summary class="foldbox__summary foldbox__summary--h3"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h3 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h3 id="three-key-takeaways">Three key takeaways</h3><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h3 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p><strong>1. Final performance doesn&#39;t depend much on rollout count.</strong> 
Both n=5 and n=64 converge to similar pass@k peaks. You&#39;re not missing out on capability by using fewer rollouts.</p>
<p><strong>2. Clear trade-off between sample and compute efficiency.</strong></p>
<ul>
<li>More rollouts (n=64) → better sample efficiency (faster convergence per training step)</li>
<li>Fewer rollouts (n=5) → better compute efficiency (similar performance with less compute)</li>
</ul>
<p><strong>3. The compute advantage is especially pronounced on hard problems.</strong>
On GSM8K-Hard (where rewards are sparse), using n=5 rollouts significantly outperforms n=64 in terms of FLOP efficiency.</p>
<p><strong>Result 3 takeaway:</strong> If you&#39;re training RL with sparse rewards, <strong>fewer rollouts can actually be more efficient</strong>. You don&#39;t need massive rollout scaling to get good performance.</p>
<!-- HTML_TAG_END --></div></div> </details></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="what-s-next">What&#39;s next?</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>This study is ongoing. Overall, we hope to consturct a controlled probe of <em>when</em> RL can help, not a full replacement recipe for pretraining.</p>
<p>Some important limitations:</p>
<ul>
<li><strong>Task scope:</strong> math reasoning with verifiable rewards is a  clean setting. </li>
<li><strong>Data mixture:</strong> our base model is pretrained on a corpus that includes a substantial fraction of math (20%) /other reasoning-related content (30%); “RL readiness” may shift with pretraining mix.</li>
<li><strong>Model scale:</strong> results are from a 1B model; larger models may show different transitions.</li>
<li><strong>Algorithm scope:</strong> we used RLVR with GRPO; other RL algorithms or denser rewards (e.g., process reward) could change the picture.</li>
</ul>
<p>Open directions we’re excited about:</p>
<ul>
<li><strong>Mixing RL into pretraining:</strong> can we interleave RL and next-token prediction in a stable way?</li>
<li><strong>Curricula for early RL:</strong> can we schedule task difficulty (or reward shaping) so early checkpoints don’t get stuck?</li>
<li><strong>Understanding expansion vs sharpening:</strong> when does RL discover new modes vs collapse onto existing ones?</li>
<li><strong>Generalization beyond math:</strong> what are the right “verifiable rewards” in other domains?</li>
</ul>
<hr>
<hr>
<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="appendix-additional-training-curves-and-ablations">Appendix: additional training curves and ablations</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>These plots are useful for you to sanity-check training stability and evaluation choices.</p>
<details>
<summary><strong>RL training convergence across checkpoints (Figure 6)</strong></summary>

<figure>
  <img src="../rl-excursions-during-pretraining/assets/figures/gsm8k_rl_sft_comparison.png" alt="RL train/val reward and GSM8K pass@1 over RL steps for multiple pretraining checkpoints." width="100%"/>
  <figcaption><strong>Figure 6.</strong> RL reward curves (train/val) and GSM8K pass@1 over RL steps show convergence across checkpoints.</figcaption>
</figure>

</details>

<details>
<summary><strong>SFT convergence (Figure 8)</strong></summary>

<figure>
  <img src="../rl-excursions-during-pretraining/assets/figures/gsm8k_sft_epoch_comparison.png" alt="SFT epoch comparison (5 vs 10 epochs) showing convergence across checkpoints on GSM8K pass@k." width="100%"/>
  <figcaption><strong>Figure 8.</strong> SFT epoch ablation indicates performance converges by ~5 epochs.</figcaption>
</figure>

</details>

<details>
<summary><strong>How we evaluate base checkpoints (Figure 9)</strong></summary>

<figure>
  <img src="../rl-excursions-during-pretraining/assets/figures/gsm8k_base_eval_shots.png" alt="n-shot prompting ablation (0/1/8-shot) for evaluating base checkpoints on GSM8K and MATH pass@k." width="100%"/>
  <figcaption><strong>Figure 9.</strong> Few-shot prompting ablation for base checkpoints: 8-shot yields the strongest evaluation performance.</figcaption>
</figure>

</details>

<!-- HTML_TAG_END --></div></div> </details><details class="foldbox foldbox--h2 svelte-1cybysc" data-h2fold="1" open><summary class="foldbox__summary foldbox__summary--h2"><span class="foldbox__caret" aria-hidden="true"></span> <span class="foldbox__h2 md-output svelte-1cybysc"><!-- HTML_TAG_START --><h2 id="citation">Citation</h2><!-- HTML_TAG_END --> </span></summary> <div class="foldbox__body foldbox__body--h2 svelte-1cybysc"><div class="md-output svelte-1cybysc"><!-- HTML_TAG_START --><p>Please cite this work as:</p>
<pre><code class="language-bibtex">@misc{rbcmsq2026rlexcursions,
  author={Rachit Bansal* and Clara Mohri* and Tian (Sunny) Qin* and David Alvarez-Melis and Sham Kakade},
  title={RL Excursions During Pre-Training: How Early Is Too Early for On-Policy Learning?},
  howpublished={url{https://rachitbansal.github.io/rl-excursions/}},
  year={2026}
}
</code></pre>
<!-- HTML_TAG_END --></div></div> </details></div> </div> </div></div></div></main>  
			
			<script>
				{
					__sveltekit_66d053 = {
						base: new URL("..", location).pathname.slice(0, -1),
						assets: "/rl-excursions-during-pretraining"
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("../_app/immutable/entry/start.BE5RMp52.js"),
						import("../_app/immutable/entry/app.DQ2muWfq.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 3],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
  </body>
</html>
